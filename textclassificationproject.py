# -*- coding: utf-8 -*-
"""TextClassificationProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11mhWRsC1EzLlgj-YBjXcu3VciBr7amws
"""

# Install 'datasets' library for loading AG News dataset
!pip install datasets

# Text processing libraries
import nltk
import string
# Dataset handling
import pandas as pd
from datasets import load_dataset
# Feature extraction
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
# Train/test split
from sklearn.model_selection import train_test_split, GridSearchCV
# Evaluation metrics
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
# Models
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
# NLP tools from NLTK
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
# Visualization
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import seaborn as sns
# Utility
import numpy as np
import joblib

# Download required NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')
# Initialize stopword set and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Function to lowercase and remove punctuation from text
def preprocess(text):
  text = text.lower() # Convert to lowercase
  text = ''.join([char for char in text if char not in string.punctuation]) # Remove punctuation
  return text

# Load AG News dataset (4-class text classification) from Hugging Face Datasets
dataset = load_dataset('ag_news')
# Extract text and labels from the training split
texts = [item['text'] for item in dataset['train']]
labels = [item['label'] for item in dataset['train']]

# Apply preprocessing to all training texts
cleaned_texts = [preprocess(t) for t in texts]

# Split dataset: 75% training, 25% testing
X_train, X_test, y_train, y_test = train_test_split(cleaned_texts, labels, test_size=0.25, random_state=42)

# Convert raw text to numerical features using TF-IDF vectorization
vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,1), min_df=5, max_df=0.85)
# Fit on training data and transform both train and test data
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Define class labels for AG News dataset
class_names = ['World', 'Sports', 'Business', 'Sci/Tech']

# Loop through each class and generate a word cloud based on average TF-IDF score
for class_id in range(4):
    # Select all documents belonging to the current class
    class_docs = [X_train[i] for i in range(len(y_train)) if y_train[i] == class_id]

    # Recompute TF-IDF features for documents in this class only
    tfidf = TfidfVectorizer(max_features=1000)
    X_tfidf = tfidf.fit_transform(class_docs)

    # Compute average TF-IDF score for each word across documents
    mean_tfidf = np.asarray(X_tfidf.mean(axis=0)).flatten()
    top_words = dict(zip(tfidf.get_feature_names_out(), mean_tfidf))

    # Create a word cloud from the top TF-IDF words
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(top_words)

    # Plot the word cloud
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis("off")
    plt.title(f"Top Words in {class_names[class_id]} Class")
    plt.show()

# Initialize the Multinomial Naive Bayes classifier
nb_classifier = MultinomialNB()

# Train the Naive Bayes classifier on the TF-IDF vectorized training data and corresponding labels
nb_classifier.fit(X_train_vec, y_train)

# Predict the class labels for the test data using the trained Naive Bayes model
y_pred = nb_classifier.predict(X_test_vec)

# Classification Report for Naive Bayes
report = classification_report(y_test, y_pred, target_names=['World', 'Sports', 'Business', 'Sci/Tech'])
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n")
print(report)

# Confusion matrix for Naive Bayes
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['World', 'Sports', 'Business', 'Sci/Tech'])
disp.plot(cmap='Blues')
plt.title("Confusion Matrix")
plt.show()

# Error analysis for Naive Bayes

# Identify misclassified indices for Naive Bayes model
misclassified_nb = np.where(np.array(y_test) != np.array(y_pred))[0]
# Display 5 example misclassified texts from Naive Bayes predictions
print(f"\n[Naive Bayes] Showing 5 misclassified examples ({len(misclassified_nb)} total):\n")
for i in misclassified_nb[:5]:
    print(f"Text: {X_test[i][:300]}...")
    print(f"True Label: {y_test[i]} | Predicted: {y_pred[i]}\n")

# Hyperparameter tuning for Naive Bayes

# Define a grid of alpha values to search for the best Naive Bayes model
nb_params = {'alpha': [0.01, 0.1, 0.5, 1, 5]}
# Perform grid search with 5-fold cross-validation to find the best alpha
nb_grid = GridSearchCV(MultinomialNB(), nb_params, cv=5, scoring='accuracy')
nb_grid.fit(X_train_vec, y_train)

# Tuned Naive Bayes Classification Report

# Output the best alpha value found and evaluate the tuned Naive Bayes model
print("Best parameters for Naive Bayes:", nb_grid.best_params_)
best_nb_model = nb_grid.best_estimator_
# Predict test labels using the tuned Naive Bayes model
y_pred_best_nb = best_nb_model.predict(X_test_vec)
print("Tuned Naive Bayes Classification Report:")
# Display classification report for the tuned model
print(classification_report(y_test, y_pred_best_nb, target_names=['World', 'Sports', 'Business', 'Sci/Tech']))

# Initialize the Logistic Regression model with a higher iteration limit for better convergence
lr_model = LogisticRegression(max_iter=1000)
# Train the Logistic Regression model on the TF-IDF features
lr_model.fit(X_train_vec, y_train)

# Predict the test labels using the trained Logistic Regression model
y_pred_lr = lr_model.predict(X_test_vec)

# Classification Report for Logistic Regression
print("Logistic Regression Classification Report:")
print(classification_report(y_test, y_pred_lr, target_names=['World', 'Sports', 'Business', 'Sci/Tech']))

# Confusion Matrix for Logistic Regression
cm_lr = confusion_matrix(y_test, y_pred_lr)
disp_lr = ConfusionMatrixDisplay(confusion_matrix=cm_lr, display_labels=['World', 'Sports', 'Business', 'Sci/Tech'])
disp_lr.plot(cmap="Greens")
plt.title("Logistic Regression Confusion Matrix")
plt.show()

# Error analysis for Logistic Regression

# Identify misclassified samples for Logistic Regression model
misclassified_lr = np.where(np.array(y_test) != np.array(y_pred_lr))[0]
# Print first 5 misclassified examples from Logistic Regression output
print(f"\n[Logistic Regression] Showing 5 misclassified examples ({len(misclassified_lr)} total):\n")
for i in misclassified_lr[:5]:
    print(f"Text: {X_test[i][:300]}...")
    print(f"True Label: {y_test[i]} | Predicted: {y_pred_lr[i]}\n")

# Hyperparameter Tuning for Logistic Regression Model

# Define grid of regularization strengths for Logistic Regression
param_grid = {'C': [0.01, 0.1, 1, 10, 100]}
# Perform grid search with 5-fold cross-validation to find best C for LR
grid = GridSearchCV(LogisticRegression(max_iter=1000), param_grid, cv=5, scoring='accuracy')
grid.fit(X_train_vec, y_train)

# Tuned Logistic Regression Classification Report

# Get best C and evaluate the tuned Logistic Regression model
print("Best parameters for Logistic Regression:", grid.best_params_)
best_lr_model = grid.best_estimator_
# Predict using tuned LR model and print evaluation metrics
y_pred_best_lr = best_lr_model.predict(X_test_vec)
print("Tuned Logistic Regression Classification Report:")
print(classification_report(y_test, y_pred_best_lr, target_names=['World', 'Sports', 'Business', 'Sci/Tech']))

# Initialize a Linear Support Vector Machine (SVM) model with increased iterations for complex data
svm_model = LinearSVC(max_iter=10000)
# Train the SVM model on the TF-IDF feature vectors
svm_model.fit(X_train_vec, y_train)
# Predict the labels of the test data using the trained SVM model
y_pred_svm = svm_model.predict(X_test_vec)

# Classification Report for SVM
print("SVM Classification Report:")
print(classification_report(y_test, y_pred_svm, target_names=['World', 'Sports', 'Business', 'Sci/Tech']))

# Confusion Matrix for SVM
cm_svm = confusion_matrix(y_test, y_pred_svm)
disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=['World', 'Sports', 'Business', 'Sci/Tech'])
disp_svm.plot(cmap="Purples")
plt.title("SVM Confusion Matrix")
plt.show()

# Error analysis for SVM

# Detect mismatches between true and predicted labels for SVM
misclassified_svm = np.where(np.array(y_test) != np.array(y_pred_svm))[0]
# Show 5 misclassified samples from SVM results
print(f"\n[SVM] Showing 5 misclassified examples ({len(misclassified_svm)} total):\n")
for i in misclassified_svm[:5]:
    print(f"Text: {X_test[i][:300]}...")
    print(f"True Label: {y_test[i]} | Predicted: {y_pred_svm[i]}\n")

# Hyperparameter tuning for SVM
# Define a parameter grid for SVM's regularization parameter C
svm_params = {'C': [0.01, 0.1, 1, 10, 100]}
# Use GridSearchCV to find best SVM hyperparameter via 5-fold CV
svm_grid = GridSearchCV(LinearSVC(max_iter=10000), svm_params, cv=5, scoring='accuracy')
svm_grid.fit(X_train_vec, y_train)

# Tuned SVM Classification Report
# Report best parameters and evaluate tuned SVM
print("Best parameters for SVM:", svm_grid.best_params_)
best_svm_model = svm_grid.best_estimator_
# Predict test labels using tuned SVM
y_pred_best_svm = best_svm_model.predict(X_test_vec)
print("Tuned SVM Classification Report:")
print(classification_report(y_test, y_pred_best_svm, target_names=['World', 'Sports', 'Business', 'Sci/Tech']))

# # Compute accuracy scores for untuned models
nb_acc = accuracy_score(y_test, y_pred)
lr_acc = accuracy_score(y_test, y_pred_lr)
svm_acc = accuracy_score(y_test, y_pred_svm)

# Compute accuracy scores for tuned models
tuned_nb_acc = accuracy_score(y_test, y_pred_best_nb)
tuned_lr_acc = accuracy_score(y_test, y_pred_best_lr)
tuned_svm_acc = accuracy_score(y_test, y_pred_best_svm)

# Print individual model accuracies for comparison
print(f"Multinomial Naive Bayes Accuracy: {nb_acc:.4f}")
print(f"Logistic Regression Accuracy: {lr_acc:.4f}")
print(f"SVM Accuracy: {svm_acc:.4f}")

# Display Results
# Create and display DataFrame comparing tuned and untuned model performance
results = {
    "Model": [
        "Naive Bayes (Untuned)", "Naive Bayes (Tuned)",
        "Logistic Regression (Untuned)", "Logistic Regression (Tuned)",
        "SVM (Untuned)", "SVM (Tuned)"
    ],
    "Accuracy": [
        nb_acc, tuned_nb_acc,
        lr_acc, tuned_lr_acc,
        svm_acc, tuned_svm_acc
    ]
}

results_df = pd.DataFrame(results)
results_df["Accuracy"] = results_df["Accuracy"].round(4)
display(results_df)

# Plot: Zoomed-in accuracy comparison for tuned & untuned models
plt.figure(figsize=(12, 6))
ax = sns.barplot(x="Model", y="Accuracy", data=results_df, palette="Set2")

# Show exact values on top of bars
for i, acc in enumerate(results_df["Accuracy"]):
    ax.text(i, acc + 0.0005, f"{acc:.4f}", ha='center', va='bottom', fontsize=10)

# Zoom in on the accuracy range to highlight small differences
plt.ylim(0.88, 0.92)
plt.title("Model Accuracy Comparison (Tuned vs Untuned) â€“ Zoomed In", fontsize=14)
plt.ylabel("Accuracy", fontsize=12)
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()